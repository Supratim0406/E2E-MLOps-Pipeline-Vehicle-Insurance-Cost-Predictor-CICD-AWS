# End to End MLOps Project - Vehicle Insurance Data Pipeline

Welcome to this MLOps project, designed to demonstrate a robust pipeline for managing vehicle insurance data. This project aims to impress recruiters and visitors by showcasing the various tools, techniques, services, and features that go into building and deploying a machine learning pipeline for real-world data management. Follow along to learn about project setup, data processing, model deployment, and CI/CD automation!

---

## ğŸ“ Project Setup and Structure

```
MLOps-Project-Vehicle-Insurance/
â”‚
â”œâ”€â”€ .github/                      # GitHub configurations (CI/CD workflows, templates)
â”œâ”€â”€ .vscode/                      # VS Code workspace settings
â”‚
â”œâ”€â”€ artifact/                     # Generated artifacts from pipeline runs
â”‚   â””â”€â”€ data_ingestion/
â”‚       â””â”€â”€ ingested/
â”‚           â””â”€â”€ test.csv          # Ingested dataset (post data ingestion stage)
â”‚
â”œâ”€â”€ config/                       # Centralized configuration files
â”‚   â”œâ”€â”€ model.yaml                # Model training configuration & hyperparameters
â”‚   â””â”€â”€ schema.yaml               # Data schema validation
â”‚
â”œâ”€â”€ logs/                         # Application and pipeline logs
â”‚
â”œâ”€â”€ notebook/                     # Jupyter notebooks for EDA & experimentation
â”‚
â”œâ”€â”€ src/                          # Core source code (Production-grade)
â”‚   â”œâ”€â”€ cloud_storage/            # Cloud storage integration (S3 / artifact store)
â”‚   â”œâ”€â”€ components/               # Modular ML pipeline components
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”‚   â”œâ”€â”€ data_validation.py
â”‚   â”‚   â”œâ”€â”€ data_transformation.py
â”‚   â”‚   â”œâ”€â”€ model_trainer.py
â”‚   â”‚   â””â”€â”€ model_evaluation.py
â”‚   â”‚
â”‚   â”œâ”€â”€ configuration/            # Configuration managers
â”‚   â”œâ”€â”€ constants/                # Global constants
â”‚   â”œâ”€â”€ data_access/              # Data access layer
â”‚   â”œâ”€â”€ entity/                   # Pydantic / dataclass entities
â”‚   â”œâ”€â”€ exception/                # Custom exception handling
â”‚   â”œâ”€â”€ logger/                   # Centralized logging setup
â”‚   â”œâ”€â”€ pipeline/                 # End-to-end training & prediction pipelines
â”‚   â”œâ”€â”€ templates/                # HTML templates (if UI is used)
â”‚   â””â”€â”€ utils/                    # Utility & helper functions
â”‚
â”œâ”€â”€ static/                       # Static files (CSS, JS, images)
â”œâ”€â”€ templates/                    # Frontend HTML templates
â”‚   â””â”€â”€ index.html
â”‚
â”œâ”€â”€ vehicle/                      # Packaged Python module
â”‚
â”œâ”€â”€ app.py                        # Application entry point (FastAPI / Flask)
â”œâ”€â”€ demo.py                       # Demo / local testing script
â”‚
â”œâ”€â”€ Dockerfile                    # Docker image definition
â”œâ”€â”€ .dockerignore                 # Docker ignore rules
â”‚
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ setup.py                      # Package setup file
â”œâ”€â”€ pyproject.toml                # Build system configuration
â”œâ”€â”€ projectflow.txt               # Project workflow documentation
â”œâ”€â”€ crashcourse.txt               # Learning notes / references
â”œâ”€â”€ LICENSE                       # License information
â””â”€â”€ README.md                     # Project documentation

```

### Step 1: Project Template
- Start by executing the `template.py` file to create the initial project template, which includes the required folder structure and placeholder files.

### Step 2: Package Management
- Write the setup for importing local packages in `setup.py` and `pyproject.toml` files.
- **Tip**: Learn more about these files from `crashcourse.txt`.

### Step 3: Virtual Environment and Dependencies
- Create a virtual environment and install required dependencies from `requirements.txt`:
  ```bash
  conda create -n vehicle python=3.10 -y
  conda activate vehicle
  pip install -r requirements.txt
  ```
- Verify the local packages by running:
  ```bash
  pip list
  ```

---

## ğŸ“Š MongoDB Setup and Data Management

### Step 4: MongoDB Atlas Configuration
1. Sign up for [MongoDB Atlas](https://www.mongodb.com/cloud/atlas) and create a new project.
2. Set up a free M0 cluster, configure the username and password, and allow access from any IP address (`0.0.0.0/0`).
3. Retrieve the MongoDB connection string for Python and save it (replace `<password>` with your password).

### Step 5: Pushing Data to MongoDB
1. Create a folder named `notebook`, add the dataset, and create a notebook file `mongoDB_demo.ipynb`.
2. Use the notebook to push data to the MongoDB database.
3. Verify the data in MongoDB Atlas under Database > Browse Collections.

---

## ğŸ“ Logging, Exception Handling, and EDA

### Step 6: Set Up Logging and Exception Handling
- Create logging and exception handling modules. Test them on a demo file `demo.py`.

### Step 7: Exploratory Data Analysis (EDA) and Feature Engineering
- Analyze and engineer features in the `EDA` and `Feature Engg` notebook for further processing in the pipeline.

---

## ğŸ“¥ Data Ingestion

### Step 8: Data Ingestion Pipeline
- Define MongoDB connection functions in `configuration.mongo_db_connections.py`.
- Develop data ingestion components in the `data_access` and `components.data_ingestion.py` files to fetch and transform data.
- Update `entity/config_entity.py` and `entity/artifact_entity.py` with relevant ingestion configurations.
- Run `demo.py` after setting up MongoDB connection as an environment variable.

### Setting Environment Variables
- Set MongoDB URL:
  ```bash
  # For Bash
  export MONGODB_URL="mongodb+srv://<username>:<password>...."
  # For Powershell
  $env:MONGODB_URL = "mongodb+srv://<username>:<password>...."
  ```
- **Note**: On Windows, you can also set environment variables through the system settings.

---

## ğŸ” Data Validation, Transformation & Model Training

### Step 9: Data Validation
- Define schema in `config.schema.yaml` and implement data validation functions in `utils.main_utils.py`.

### Step 10: Data Transformation
- Implement data transformation logic in `components.data_transformation.py` and create `estimator.py` in the `entity` folder.

### Step 11: Model Training
- Define and implement model training steps in `components.model_trainer.py` using code from `estimator.py`.

---

## ğŸŒ AWS Setup for Model Evaluation & Deployment

### Step 12: AWS Setup
1. Log in to the AWS console, create an IAM user, and grant `AdministratorAccess`.
2. Set AWS credentials as environment variables.
   ```bash
   # For Bash
   export AWS_ACCESS_KEY_ID="YOUR_AWS_ACCESS_KEY_ID"
   export AWS_SECRET_ACCESS_KEY="YOUR_AWS_SECRET_ACCESS_KEY"
   ```

3. Configure S3 Bucket and add access keys in `constants.__init__.py`.

### Step 13: Model Evaluation and Pushing to S3
- Create an S3 bucket named `my-model-mlopsproj` in the `us-east-1` region.
- Develop code to push/pull models to/from the S3 bucket in `src.aws_storage` and `entity/s3_estimator.py`.

---

## ğŸš€ Model Evaluation, Model Pusher, and Prediction Pipeline

### Step 14: Model Evaluation & Model Pusher
- Implement model evaluation and deployment components.
- Create `Prediction Pipeline` and set up `app.py` for API integration.

### Step 15: Static and Template Directory
- Add `static` and `template` directories for web UI.

---

## ğŸ”„ CI/CD Setup with Docker, GitHub Actions, and AWS

### Step 16: Docker and GitHub Actions
1. Create `Dockerfile` and `.dockerignore`.
2. Set up GitHub Actions with AWS authentication by creating secrets in GitHub for:
   - `AWS_ACCESS_KEY_ID`
   - `AWS_SECRET_ACCESS_KEY`
   - `AWS_DEFAULT_REGION`
   - `ECR_REPO`

### Step 17: AWS EC2 and ECR
1. Set up an EC2 instance for deployment.
2. Install Docker on the EC2 machine.
3. Connect EC2 as a self-hosted runner on GitHub.

### Step 18: Final Steps
1. Open the 5080 port on the EC2 instance.
2. Access the deployed app by visiting `http://<public_ip>:5080`.

### Demo video: 
[Demo Video](https://github.com/user-attachments/assets/75437085-4e38-4fde-b8f7-09d8d2aa3e41)

---

## ğŸ› ï¸ Additional Resources
- **Crash Course on setup.py and pyproject.toml**: See `crashcourse.txt` for details.
- **GitHub Secrets**: Manage secrets for secure CI/CD pipelines.

---

## ğŸ¯ Project Workflow Summary

1. **Data Ingestion** â” **Data Validation** â” **Data Transformation**
2. **Model Training** â” **Model Evaluation** â” **Model Deployment**
3. **CI/CD Automation** with GitHub Actions, Docker, AWS EC2, and ECR

---

## ğŸ’¬ Connect
If you found this project helpful or have any questions, feel free to reach out!

---

This README provides a structured walkthrough of the MLOps project, showcasing the end-to-end pipeline, cloud integration, CI/CD setup, and robust data handling capabilities.

